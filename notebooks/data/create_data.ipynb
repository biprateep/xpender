{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that this notebook is running on the kernel with the legacy survey cutout service + dask docker image.\n",
    "\n",
    "To set up the Dask cluster, run following cell in Jupyter terminal\n",
    "\n",
    "`salloc -N 4 -n 512 -t 240 -C cpu -q interactive --image=biprateep/dask-viewer-cutouts:latest --account=m4236`\n",
    "\n",
    "and then \n",
    "\n",
    "`./launch_dask.sh` \n",
    "\n",
    "the `-n` argument controls the number of workers to be launched.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the spectra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fitsio\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from desiutil.io import encode_table\n",
    "from desiutil.log import get_logger, DEBUG\n",
    "from desispec.io.util import native_endian, checkgzip\n",
    "from desispec.io import iotime\n",
    "from tqdm import tqdm\n",
    "import zarr\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask.distributed import LocalCluster\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the scheduler file generated by the script above and connect your notebook to the client\n",
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler.json\")\n",
    "# scheduler_file = os.path.join(os.environ[\"CFS\"], \"desi/users/bid13/scheduler.json\")\n",
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\"\n",
    "client = Client(scheduler_file=scheduler_file)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(threads_per_worker=1)\n",
    "# client = cluster.get_client()\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"iron\"\n",
    "dest_path = Path(os.environ[\"SCRATCH\"]) / \"data\" / \"foundation\" / f\"{release}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcat = pd.read_parquet(dest_path / \"desi_zcat_maglim_19_5.parquet\", columns = [\"SURVEY\",\"PROGRAM\",\"HEALPIX\",\"TARGETID\",\"MYID\"])\n",
    "zcat = zcat.set_index(\"MYID\")\n",
    "zcat = zcat.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolution_coadd(resolution, pix_weights):\n",
    "    \"\"\"\n",
    "    Given the resolution matrices for set of spectra, and\n",
    "    inverse variances (or generally weights) for fluxes return the\n",
    "    accumulated resolution matrix, and the combined weights\n",
    "    See #2372.\n",
    "\n",
    "    Args:\n",
    "    resolution (ndarray): (nspec, nres, npix) array of resolution matrices\n",
    "    pix_weights (ndarray): (nspec, npix) array of ivars or weights\n",
    "\n",
    "    Returns resolution matrix (nres, npix),\n",
    "    and the weight (nres, npix)\n",
    "    \"\"\"\n",
    "    ww = resolution.shape[1] // 2\n",
    "    # resolution kernel width\n",
    "    npix = resolution.shape[2]\n",
    "    # indices of the corresponding variance point\n",
    "    # that needs to be used for ivar weights\n",
    "    res_indices = (np.arange(npix)[None, :] +\n",
    "                   np.arange(-ww, ww + 1)[:, None]) % npix\n",
    "    res_whts = np.array([_[res_indices] for _ in pix_weights])\n",
    "    res = np.sum(res_whts * resolution, axis=0)\n",
    "    res_norm = np.sum(res_whts, axis=0)\n",
    "    return res, res_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coadd_cameras(flux_cam, wave_cam, ivar_cam, mask_cam, res_cam):\n",
    "    \n",
    "\n",
    "    sbands = np.array([\"b\", \"r\", \"z\"])  # bands sorted by inc. wavelength\n",
    "    # create wavelength array\n",
    "    wave = None\n",
    "    tolerance = 0.0001  # A , tolerance\n",
    "  \n",
    "    windict = {}\n",
    "\n",
    "    for b in sbands:\n",
    "        if wave is None:\n",
    "            wave = wave_cam[b]\n",
    "        else:\n",
    "            wave = np.append(wave, wave_cam[b][wave_cam[b] > wave[-1] + tolerance])\n",
    "\n",
    "    # check alignment, caching band wavelength grid indices as we go\n",
    "    for b in sbands:\n",
    "        imin = np.argmin(np.abs(wave_cam[b][0] - wave))\n",
    "        windices = np.arange(imin, imin + len(wave_cam[b]), dtype=int)\n",
    "        dwave = wave_cam[b] - wave[windices]\n",
    "        if np.any(np.abs(dwave) > tolerance):\n",
    "            msg = \"Input wavelength grids (band '{}') are not aligned. Use --lin-step or --log10-step to resample to a common grid.\".format(\n",
    "                b)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    nwave = wave.size\n",
    "\n",
    "    # creating a dictionary for each band to assign\n",
    "    # which pixels are overlapping with other bands\n",
    "    # it masked life easier tracking everything when normalizing the pixels\n",
    "    overlap_flag = {}\n",
    "    for i, b in enumerate(sbands):\n",
    "        wave_b = wave_cam[b]\n",
    "        flag = np.zeros_like(wave_b, dtype=int)\n",
    "\n",
    "        # Check overlap with previous band\n",
    "        if i > 0:\n",
    "            wave_prev = wave_cam[sbands[i - 1]]\n",
    "            # Mark overlapping pixels in current band\n",
    "            for j, w in enumerate(wave_b):\n",
    "                if np.any(np.abs(w - wave_prev) <= tolerance):\n",
    "                    flag[j] = 1\n",
    "\n",
    "        # Check overlap with next band\n",
    "        if i < len(sbands) - 1:\n",
    "            wave_next = wave_cam[sbands[i + 1]]\n",
    "            for j, w in enumerate(wave_b):\n",
    "                if np.any(np.abs(w - wave_next) <= tolerance):\n",
    "                    flag[j] = 1\n",
    "\n",
    "        overlap_flag[b] = flag\n",
    "\n",
    "    # defining arrays for coadded data\n",
    "    flux = np.zeros((1, nwave))\n",
    "    ivar = np.zeros((1, nwave))\n",
    "    mask = np.zeros((1, nwave), dtype=np.int32)\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    max_ndiag = max([res_cam[b].shape[1] for b in sbands])\n",
    "    rdata = np.zeros((1, max_ndiag, nwave))\n",
    "    rnorm = np.zeros_like(rdata)\n",
    "\n",
    "    for b in sbands:\n",
    "        \n",
    "        wband = wave_cam[b]\n",
    "        start = np.searchsorted(wave, wband[0])\n",
    "        end = start + len(wband)\n",
    "        iband = slice(start, end)\n",
    "        windict[b] = iband\n",
    "        no_overlap = (overlap_flag[b]==0)\n",
    "\n",
    "        f = flux_cam[b]\n",
    "        iv = ivar_cam[b]\n",
    "        m = mask_cam[b] \n",
    "\n",
    "        # True for pixels in b that are non-overlapping\n",
    "        no_overlap = (overlap_flag[b] == 0)\n",
    "\n",
    "        \n",
    "        # Non-overlapping: directly copy\n",
    "        flux[0, iband][no_overlap] = f[0][no_overlap]\n",
    "        ivar[0, iband][no_overlap] = iv[0][no_overlap]\n",
    "\n",
    "        # Overlapping: accumulate (inverse variance weighted sum)\n",
    "        overlap = ~no_overlap\n",
    "\n",
    "        # coadding flux and ivar\n",
    "        flux[0, iband][overlap] += iv[0][overlap] * f[0][overlap]\n",
    "        ivar[0, iband][overlap] += iv[0][overlap]\n",
    "\n",
    "        # for masks, models and resolution matrix\n",
    "        # (in no overlapping regions, simple copying)\n",
    "        # in overlapping regions, inverse variance weighted mean\n",
    "        \n",
    "        # coadding mask\n",
    "        mask[0, iband][no_overlap] = m[0][no_overlap] # non-overlapping, simple copy\n",
    "        mask[0, iband][overlap] |= m[0][overlap] # overlapping, OR logic\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        res = res_cam[b][0][np.newaxis, :, :]\n",
    "        iv_i = iv[0:0+1]\n",
    "        raccum, rnorm_i = _resolution_coadd(res, iv_i)\n",
    "        ndiag = raccum.shape[0]\n",
    "        offset = (max_ndiag - ndiag) // 2\n",
    "\n",
    "        # non-overlapping regions, simple copying\n",
    "        rdata[0, offset:offset+ndiag, iband.start:iband.stop][:, no_overlap] = res[0][:, no_overlap]\n",
    "        rnorm[0, offset:offset+ndiag, iband.start:iband.stop][:, no_overlap] = 1.0\n",
    "\n",
    "        # non-overlapping regions, weighted mean\n",
    "        rdata[0, offset:offset+ndiag, iband.start:iband.stop][:, overlap] += raccum[:, overlap]\n",
    "        rnorm[0, offset:offset+ndiag, iband.start:iband.stop][:, overlap] += rnorm_i[:, overlap]\n",
    "\n",
    "    # in the combined unique wave pixels\n",
    "    # which pixels have two measurements due to overlapping\n",
    "    overlap_pixel_mask = np.zeros_like(flux, dtype=bool)\n",
    "    for b in sbands:\n",
    "        band_indices = np.arange(windict[b].start, windict[b].stop)\n",
    "        overlap_pixel_mask[:, band_indices] = overlap_flag[b][None, :]\n",
    "\n",
    "    # Only normalize on overlapping pixels (basically inverse variance weighted mean)\n",
    "    # For non-overlapping (already direct copied), skip normalization\n",
    "    normalize_mask = (overlap_pixel_mask == 1)\n",
    "    flux[normalize_mask] /= (ivar[normalize_mask] + (ivar[normalize_mask] == 0))\n",
    "\n",
    "    mask[ivar > 0] = 0 # mask =0 means good pixels\n",
    "\n",
    "    ivar[mask.astype(bool)] = 0 # encoding all mask values in ivar\n",
    "   \n",
    "\n",
    "    # just sanity chack that wavelength is an increasing array\n",
    "    assert np.all(np.diff(wave) > 0)\n",
    "\n",
    "    \n",
    "    rdata_norm_pixels = normalize_mask[0] # all rows of normalize mask are basically same\n",
    "    rdata[:, :, rdata_norm_pixels] /= rnorm[:, :, rdata_norm_pixels] + (rnorm[:, :, rdata_norm_pixels] == 0)\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "   \n",
    "    return flux, wave, ivar, mask, rdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spectra(survey, program, healpix, targetid, release=\"iron\",read_hdu={\n",
    "            \"FIBERMAP\": True,\n",
    "            \"EXP_FIBERMAP\": False,\n",
    "            \"SCORES\": False,\n",
    "            \"EXTRA_CATALOG\": False,\n",
    "            \"MASK\": False,\n",
    "            \"RESOLUTION\": True,\n",
    "        }):\n",
    "    release_path = Path( f\"/global/cfs/cdirs/desi/spectro/redux/{release}\")\n",
    "   \n",
    "    infile = (\n",
    "       release_path \n",
    "        / \"healpix\"\n",
    "        / survey\n",
    "        / program\n",
    "        / str(int(healpix / 100))\n",
    "        / str(healpix)\n",
    "        / f\"coadd-{survey}-{program}-{healpix}.fits\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # log = get_logger()\n",
    "    ftype = np.float32\n",
    "\n",
    "    # t0 = time.time()\n",
    "    hdus = fitsio.FITS(infile, mode='r')\n",
    "\n",
    "    targetrow = np.argwhere(hdus[\"FIBERMAP\"].read(columns=\"TARGETID\")==targetid)[0][0]\n",
    "    nhdu = len(hdus)\n",
    "\n",
    "    # load the metadata.\n",
    "\n",
    "    meta = dict(hdus[0].read_header())\n",
    "\n",
    "    # initialize data objects\n",
    "\n",
    "    bands = []\n",
    "    fmap = None\n",
    "    expfmap = None\n",
    "    wave = None\n",
    "    flux = None\n",
    "    ivar = None\n",
    "    mask = None\n",
    "    res = None\n",
    "    extra = None\n",
    "    extra_catalog = None\n",
    "    scores = None\n",
    "\n",
    "    # For efficiency, go through the HDUs in disk-order.  Use the\n",
    "    # extension name to determine where to put the data.  We don't\n",
    "    # explicitly copy the data, since that will be done when constructing\n",
    "    # the Spectra object.\n",
    "            \n",
    "    for h in range(1, nhdu):\n",
    "        name = hdus[h].read_header()[\"EXTNAME\"]\n",
    "        if name == \"FIBERMAP\":\n",
    "            pass\n",
    "        elif name == \"EXP_FIBERMAP\":\n",
    "            pass\n",
    "        elif name == \"SCORES\":\n",
    "            pass\n",
    "        elif name == \"EXTRA_CATALOG\":\n",
    "            pass\n",
    "        else:\n",
    "            # Find the band based on the name\n",
    "            mat = re.match(r\"(.*)_(.*)\", name)\n",
    "            if mat is None:\n",
    "                raise RuntimeError(\"FITS extension name {} does not contain the band\".format(name))\n",
    "            band = mat.group(1).lower()\n",
    "            type = mat.group(2)\n",
    "            if band not in bands:\n",
    "                bands.append(band)\n",
    "            if type == \"WAVELENGTH\":\n",
    "                if wave is None:\n",
    "                    wave = {}\n",
    "                #- Note: keep original float64 resolution for wavelength\n",
    "                wave[band] = native_endian(hdus[h].read())\n",
    "            elif type == \"FLUX\":\n",
    "                if flux is None:\n",
    "                    flux = {}\n",
    "                flux[band] = native_endian(hdus[h][targetrow:targetrow+1, :])\n",
    "            elif type == \"IVAR\":\n",
    "                if ivar is None:\n",
    "                    ivar = {}\n",
    "                ivar[band] = native_endian(hdus[h][targetrow:targetrow+1, :])\n",
    "            elif type == \"MASK\":\n",
    "                if mask is None:\n",
    "                    mask = {}\n",
    "                mask[band] = native_endian(hdus[h][targetrow:targetrow+1, :].astype(np.uint32))\n",
    "                \n",
    "            elif type == \"RESOLUTION\" and read_hdu[\"RESOLUTION\"]:\n",
    "                if res is None:\n",
    "                    res = {}\n",
    "                res[band] = native_endian(\n",
    "                    hdus[h][targetrow : targetrow + 1, :, :]\n",
    "                )\n",
    "        \n",
    "            else:\n",
    "                pass\n",
    "    hdus.close()\n",
    "    flux, wave, ivar, mask, res = coadd_cameras(flux, wave, ivar, mask, res)\n",
    "    \n",
    "\n",
    "    return flux.astype(ftype), wave, ivar.astype(ftype), mask, res.astype(ftype) # use fp 16 for Res?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size = 1000): \n",
    "    chunks = list()\n",
    "    num_chunks = int(np.ceil(len(df) / chunk_size))\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(nout=3)\n",
    "def get_spectra(params:pd.DataFrame) -> np.array:\n",
    "    fluxes = list()\n",
    "    ivars = list()\n",
    "    reses = list()\n",
    "    for i, p in params.iterrows():\n",
    "        flux, wave, ivar, mask, res = read_spectra(p[\"SURVEY\"],p[\"PROGRAM\"],p[\"HEALPIX\"],p[\"TARGETID\"])\n",
    "        fluxes.append(flux)\n",
    "        ivars.append(ivar)\n",
    "        reses.append(res)\n",
    "    \n",
    "    return np.concatenate(fluxes), np.concatenate(ivars), np.concatenate(reses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_params = split_dataframe(zcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fluxes = []\n",
    "all_ivars = []\n",
    "all_reses = []\n",
    "\n",
    "for i in tqdm(chunk_params):\n",
    "    fluxes, ivars, reses = get_spectra(i)\n",
    "    all_fluxes.append(da.from_delayed(fluxes,dtype=np.float32, shape=(len(i),7781)))\n",
    "    all_ivars.append(da.from_delayed(ivars,dtype=np.float32, shape=(len(i),7781)))\n",
    "    all_reses.append(da.from_delayed(reses,dtype=np.float32, shape=(len(i),11,7781)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fluxes =  da.concatenate(all_fluxes)\n",
    "all_ivars =  da.concatenate(all_ivars)\n",
    "all_reses = da.concatenate(all_reses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = dest_path / \"desi_maglim_19_5.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    #TODO: Move the kwargs to match the updated API\n",
    "    all_fluxes.to_zarr(str(filepath),overwrite=True, compressor=None,component=\"FLUX\") \n",
    "    all_ivars.to_zarr(str(filepath),overwrite=True, compressor=None,component=\"IVAR\")\n",
    "    all_reses.to_zarr(str(filepath),overwrite=True, compressor=None,component=\"RESOLUTION\")\n",
    "    da.linspace(3600,9824,7781).to_zarr(str(filepath),overwrite=True, compressor=None,component=\"WAVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Assurance\n",
    "Spot check by retrieving a sample spectra from the saved file and compare with spectra on the disk\n",
    "and legacy survey viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import zarr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release = \"iron\"\n",
    "dest_path = Path(os.environ[\"SCRATCH\"]) / \"data\" / \"foundation\" / f\"{release}\"\n",
    "\n",
    "\n",
    "zcat = pd.read_parquet(dest_path / \"desi_zcat_maglim_19_5.parquet\", columns = [\"SURVEY\",\"PROGRAM\",\"HEALPIX\",\"TARGETID\",\"MYID\"])\n",
    "\n",
    "spec = zarr.open(dest_path / \"desi_maglim_19_5.zarr\", mode=\"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 67990\n",
    "survey = zcat[\"SURVEY\"].iloc[idx]\n",
    "program = zcat[\"PROGRAM\"].iloc[idx]\n",
    "healpix = zcat[\"HEALPIX\"].iloc[idx]\n",
    "targetid = zcat[\"TARGETID\"].iloc[idx]\n",
    "flux, wave, ivar, mask, res = read_spectra(zcat[\"SURVEY\"].iloc[idx], zcat[\"PROGRAM\"].iloc[idx], zcat[\"HEALPIX\"].iloc[idx], zcat[\"TARGETID\"].iloc[idx])\n",
    "\n",
    "flux2, wave2, ivar2, mask2, res2 = spec[\"FLUX\"][idx,:], spec[\"WAVE\"][:], spec[\"IVAR\"][idx,:], None, spec[\"RESOLUTION\"][idx,:]\n",
    "print(zcat.iloc[idx])\n",
    "# print url for LS viewer\n",
    "print(f\"Web link: https://www.legacysurvey.org/viewer/desi-spectrum/dr1/targetid{targetid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(wave,np.squeeze(flux))\n",
    "plt.plot(wave,flux2, ls=\"--\",lw=0.5)\n",
    "print(f\"Squares Error:{np.mean((flux-flux2)**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(wave,np.squeeze(ivar))\n",
    "plt.plot(wave,ivar2, ls=\"--\",lw=0.5)\n",
    "print(f\"Squares Error:{np.mean((ivar-ivar2)**2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(10,5))\n",
    "ax[0].matshow(np.squeeze(res))\n",
    "ax[0].set_aspect('auto')\n",
    "ax[1].matshow(np.squeeze(res2))\n",
    "ax[1].set_aspect('auto')\n",
    "print(f\"Squares Error:{np.mean((res-res2)**2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desi-dask",
   "language": "python",
   "name": "desi-dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
